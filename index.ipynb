{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Segmentation with Clustering - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we'll use our knowledge of clustering to perform market segmentation on a real-world dataset!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Identify and explain what Market Segmentation is, and how clustering can be used for segmentation\n",
    "* Use clustering algorithms to create and interpret a market segmentation on real-world data\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "In this lab, we're going to work with the [Wholesale Customers Dataset] from the UCI Machine Learning Datasets Repository. This dataset contains data on wholesale purchasing information from real businesses. These businesses range from small cafes and hotels to grocery stores and other retailers. \n",
    "\n",
    "Here's the data dictionary for this dataset:\n",
    "\n",
    "|      Column      |                                               Description                                              |\n",
    "|:----------------:|:------------------------------------------------------------------------------------------------------:|\n",
    "|       FRESH      |                    Annual spending on fresh products, such as fruits and vegetables                    |\n",
    "|       MILK       |                               Annual spending on milk and dairy products                               |\n",
    "|      GROCERY     |                                   Annual spending on grocery products                                  |\n",
    "|      FROZEN      |                                   Annual spending on frozen products                                   |\n",
    "| DETERGENTS_PAPER |                  Annual spending on detergents, cleaning supplies, and paper products                  |\n",
    "|   DELICATESSEN   |                           Annual spending on meats and delicatessen products                           |\n",
    "|      CHANNEL     | Type of customer.  1=Hotel/Restaurant/Cafe, 2=Retailer. (This is what we'll use clustering to predict) |\n",
    "|      REGION      |            Region of Portugal that the customer is located in. (This column will be dropped)           |\n",
    "\n",
    "\n",
    "\n",
    "One benefit of working with this dataset for practice with segmentation is that we actually have the ground-truth labels of what market segment each customer actually belongs to. For this reason, we'll borrow some methodology from Supervised Learning and store these labels separately, so that we can use them afterwards to check how well our clustering segmentation actually performed. \n",
    "\n",
    "Let's get started by importing everything we'll need.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Import pandas, numpy, and matplotlib.pyplot, and set the standard alias for each. \n",
    "* Use numpy to set a random seed of `0`.\n",
    "* Set all matplotlib visualizations to appear inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load our data and inspect it. You'll find the data stored in `wholesale_customers_data.csv`. \n",
    "\n",
    "In the cell below, load the data into a DataFrame and then display the head to ensure everything loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv('wholesale_customers_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's go ahead and store the `'Channel'` column in a separate variable, and then drop both the `'Channel'` and `'Region'` columns. Then, display the head of the new DataFrame to ensure everything worked correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicassen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12669</td>\n",
       "      <td>9656</td>\n",
       "      <td>7561</td>\n",
       "      <td>214</td>\n",
       "      <td>2674</td>\n",
       "      <td>1338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7057</td>\n",
       "      <td>9810</td>\n",
       "      <td>9568</td>\n",
       "      <td>1762</td>\n",
       "      <td>3293</td>\n",
       "      <td>1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6353</td>\n",
       "      <td>8808</td>\n",
       "      <td>7684</td>\n",
       "      <td>2405</td>\n",
       "      <td>3516</td>\n",
       "      <td>7844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13265</td>\n",
       "      <td>1196</td>\n",
       "      <td>4221</td>\n",
       "      <td>6404</td>\n",
       "      <td>507</td>\n",
       "      <td>1788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22615</td>\n",
       "      <td>5410</td>\n",
       "      <td>7198</td>\n",
       "      <td>3915</td>\n",
       "      <td>1777</td>\n",
       "      <td>5185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9413</td>\n",
       "      <td>8259</td>\n",
       "      <td>5126</td>\n",
       "      <td>666</td>\n",
       "      <td>1795</td>\n",
       "      <td>1451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12126</td>\n",
       "      <td>3199</td>\n",
       "      <td>6975</td>\n",
       "      <td>480</td>\n",
       "      <td>3140</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7579</td>\n",
       "      <td>4956</td>\n",
       "      <td>9426</td>\n",
       "      <td>1669</td>\n",
       "      <td>3321</td>\n",
       "      <td>2566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5963</td>\n",
       "      <td>3648</td>\n",
       "      <td>6192</td>\n",
       "      <td>425</td>\n",
       "      <td>1716</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6006</td>\n",
       "      <td>11093</td>\n",
       "      <td>18881</td>\n",
       "      <td>1159</td>\n",
       "      <td>7425</td>\n",
       "      <td>2098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3366</td>\n",
       "      <td>5403</td>\n",
       "      <td>12974</td>\n",
       "      <td>4400</td>\n",
       "      <td>5977</td>\n",
       "      <td>1744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13146</td>\n",
       "      <td>1124</td>\n",
       "      <td>4523</td>\n",
       "      <td>1420</td>\n",
       "      <td>549</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>31714</td>\n",
       "      <td>12319</td>\n",
       "      <td>11757</td>\n",
       "      <td>287</td>\n",
       "      <td>3881</td>\n",
       "      <td>2931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21217</td>\n",
       "      <td>6208</td>\n",
       "      <td>14982</td>\n",
       "      <td>3095</td>\n",
       "      <td>6707</td>\n",
       "      <td>602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>24653</td>\n",
       "      <td>9465</td>\n",
       "      <td>12091</td>\n",
       "      <td>294</td>\n",
       "      <td>5058</td>\n",
       "      <td>2168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10253</td>\n",
       "      <td>1114</td>\n",
       "      <td>3821</td>\n",
       "      <td>397</td>\n",
       "      <td>964</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1020</td>\n",
       "      <td>8816</td>\n",
       "      <td>12121</td>\n",
       "      <td>134</td>\n",
       "      <td>4508</td>\n",
       "      <td>1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5876</td>\n",
       "      <td>6157</td>\n",
       "      <td>2933</td>\n",
       "      <td>839</td>\n",
       "      <td>370</td>\n",
       "      <td>4478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18601</td>\n",
       "      <td>6327</td>\n",
       "      <td>10099</td>\n",
       "      <td>2205</td>\n",
       "      <td>2767</td>\n",
       "      <td>3181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7780</td>\n",
       "      <td>2495</td>\n",
       "      <td>9464</td>\n",
       "      <td>669</td>\n",
       "      <td>2518</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>17546</td>\n",
       "      <td>4519</td>\n",
       "      <td>4602</td>\n",
       "      <td>1066</td>\n",
       "      <td>2259</td>\n",
       "      <td>2124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5567</td>\n",
       "      <td>871</td>\n",
       "      <td>2010</td>\n",
       "      <td>3383</td>\n",
       "      <td>375</td>\n",
       "      <td>569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>31276</td>\n",
       "      <td>1917</td>\n",
       "      <td>4469</td>\n",
       "      <td>9408</td>\n",
       "      <td>2381</td>\n",
       "      <td>4334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>26373</td>\n",
       "      <td>36423</td>\n",
       "      <td>22019</td>\n",
       "      <td>5154</td>\n",
       "      <td>4337</td>\n",
       "      <td>16523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>22647</td>\n",
       "      <td>9776</td>\n",
       "      <td>13792</td>\n",
       "      <td>2915</td>\n",
       "      <td>4482</td>\n",
       "      <td>5778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16165</td>\n",
       "      <td>4230</td>\n",
       "      <td>7595</td>\n",
       "      <td>201</td>\n",
       "      <td>4003</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9898</td>\n",
       "      <td>961</td>\n",
       "      <td>2861</td>\n",
       "      <td>3151</td>\n",
       "      <td>242</td>\n",
       "      <td>833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>14276</td>\n",
       "      <td>803</td>\n",
       "      <td>3045</td>\n",
       "      <td>485</td>\n",
       "      <td>100</td>\n",
       "      <td>518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4113</td>\n",
       "      <td>20484</td>\n",
       "      <td>25957</td>\n",
       "      <td>1158</td>\n",
       "      <td>8604</td>\n",
       "      <td>5206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>43088</td>\n",
       "      <td>2100</td>\n",
       "      <td>2609</td>\n",
       "      <td>1200</td>\n",
       "      <td>1107</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>6633</td>\n",
       "      <td>2096</td>\n",
       "      <td>4563</td>\n",
       "      <td>1389</td>\n",
       "      <td>1860</td>\n",
       "      <td>1892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>2126</td>\n",
       "      <td>3289</td>\n",
       "      <td>3281</td>\n",
       "      <td>1535</td>\n",
       "      <td>235</td>\n",
       "      <td>4365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>97</td>\n",
       "      <td>3605</td>\n",
       "      <td>12400</td>\n",
       "      <td>98</td>\n",
       "      <td>2970</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>4983</td>\n",
       "      <td>4859</td>\n",
       "      <td>6633</td>\n",
       "      <td>17866</td>\n",
       "      <td>912</td>\n",
       "      <td>2435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>5969</td>\n",
       "      <td>1990</td>\n",
       "      <td>3417</td>\n",
       "      <td>5679</td>\n",
       "      <td>1135</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>7842</td>\n",
       "      <td>6046</td>\n",
       "      <td>8552</td>\n",
       "      <td>1691</td>\n",
       "      <td>3540</td>\n",
       "      <td>1874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>4389</td>\n",
       "      <td>10940</td>\n",
       "      <td>10908</td>\n",
       "      <td>848</td>\n",
       "      <td>6728</td>\n",
       "      <td>993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>5065</td>\n",
       "      <td>5499</td>\n",
       "      <td>11055</td>\n",
       "      <td>364</td>\n",
       "      <td>3485</td>\n",
       "      <td>1063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>660</td>\n",
       "      <td>8494</td>\n",
       "      <td>18622</td>\n",
       "      <td>133</td>\n",
       "      <td>6740</td>\n",
       "      <td>776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>8861</td>\n",
       "      <td>3783</td>\n",
       "      <td>2223</td>\n",
       "      <td>633</td>\n",
       "      <td>1580</td>\n",
       "      <td>1521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>4456</td>\n",
       "      <td>5266</td>\n",
       "      <td>13227</td>\n",
       "      <td>25</td>\n",
       "      <td>6818</td>\n",
       "      <td>1393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>17063</td>\n",
       "      <td>4847</td>\n",
       "      <td>9053</td>\n",
       "      <td>1031</td>\n",
       "      <td>3415</td>\n",
       "      <td>1784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>26400</td>\n",
       "      <td>1377</td>\n",
       "      <td>4172</td>\n",
       "      <td>830</td>\n",
       "      <td>948</td>\n",
       "      <td>1218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>17565</td>\n",
       "      <td>3686</td>\n",
       "      <td>4657</td>\n",
       "      <td>1059</td>\n",
       "      <td>1803</td>\n",
       "      <td>668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>16980</td>\n",
       "      <td>2884</td>\n",
       "      <td>12232</td>\n",
       "      <td>874</td>\n",
       "      <td>3213</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>11243</td>\n",
       "      <td>2408</td>\n",
       "      <td>2593</td>\n",
       "      <td>15348</td>\n",
       "      <td>108</td>\n",
       "      <td>1886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>13134</td>\n",
       "      <td>9347</td>\n",
       "      <td>14316</td>\n",
       "      <td>3141</td>\n",
       "      <td>5079</td>\n",
       "      <td>1894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>31012</td>\n",
       "      <td>16687</td>\n",
       "      <td>5429</td>\n",
       "      <td>15082</td>\n",
       "      <td>439</td>\n",
       "      <td>1163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>3047</td>\n",
       "      <td>5970</td>\n",
       "      <td>4910</td>\n",
       "      <td>2198</td>\n",
       "      <td>850</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>8607</td>\n",
       "      <td>1750</td>\n",
       "      <td>3580</td>\n",
       "      <td>47</td>\n",
       "      <td>84</td>\n",
       "      <td>2501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>3097</td>\n",
       "      <td>4230</td>\n",
       "      <td>16483</td>\n",
       "      <td>575</td>\n",
       "      <td>241</td>\n",
       "      <td>2080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>8533</td>\n",
       "      <td>5506</td>\n",
       "      <td>5160</td>\n",
       "      <td>13486</td>\n",
       "      <td>1377</td>\n",
       "      <td>1498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>21117</td>\n",
       "      <td>1162</td>\n",
       "      <td>4754</td>\n",
       "      <td>269</td>\n",
       "      <td>1328</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>1982</td>\n",
       "      <td>3218</td>\n",
       "      <td>1493</td>\n",
       "      <td>1541</td>\n",
       "      <td>356</td>\n",
       "      <td>1449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>16731</td>\n",
       "      <td>3922</td>\n",
       "      <td>7994</td>\n",
       "      <td>688</td>\n",
       "      <td>2371</td>\n",
       "      <td>838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>29703</td>\n",
       "      <td>12051</td>\n",
       "      <td>16027</td>\n",
       "      <td>13135</td>\n",
       "      <td>182</td>\n",
       "      <td>2204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>39228</td>\n",
       "      <td>1431</td>\n",
       "      <td>764</td>\n",
       "      <td>4510</td>\n",
       "      <td>93</td>\n",
       "      <td>2346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>14531</td>\n",
       "      <td>15488</td>\n",
       "      <td>30243</td>\n",
       "      <td>437</td>\n",
       "      <td>14841</td>\n",
       "      <td>1867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>10290</td>\n",
       "      <td>1981</td>\n",
       "      <td>2232</td>\n",
       "      <td>1038</td>\n",
       "      <td>168</td>\n",
       "      <td>2125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>2787</td>\n",
       "      <td>1698</td>\n",
       "      <td>2510</td>\n",
       "      <td>65</td>\n",
       "      <td>477</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>440 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Fresh   Milk  Grocery  Frozen  Detergents_Paper  Delicassen\n",
       "0    12669   9656     7561     214              2674        1338\n",
       "1     7057   9810     9568    1762              3293        1776\n",
       "2     6353   8808     7684    2405              3516        7844\n",
       "3    13265   1196     4221    6404               507        1788\n",
       "4    22615   5410     7198    3915              1777        5185\n",
       "5     9413   8259     5126     666              1795        1451\n",
       "6    12126   3199     6975     480              3140         545\n",
       "7     7579   4956     9426    1669              3321        2566\n",
       "8     5963   3648     6192     425              1716         750\n",
       "9     6006  11093    18881    1159              7425        2098\n",
       "10    3366   5403    12974    4400              5977        1744\n",
       "11   13146   1124     4523    1420               549         497\n",
       "12   31714  12319    11757     287              3881        2931\n",
       "13   21217   6208    14982    3095              6707         602\n",
       "14   24653   9465    12091     294              5058        2168\n",
       "15   10253   1114     3821     397               964         412\n",
       "16    1020   8816    12121     134              4508        1080\n",
       "17    5876   6157     2933     839               370        4478\n",
       "18   18601   6327    10099    2205              2767        3181\n",
       "19    7780   2495     9464     669              2518         501\n",
       "20   17546   4519     4602    1066              2259        2124\n",
       "21    5567    871     2010    3383               375         569\n",
       "22   31276   1917     4469    9408              2381        4334\n",
       "23   26373  36423    22019    5154              4337       16523\n",
       "24   22647   9776    13792    2915              4482        5778\n",
       "25   16165   4230     7595     201              4003          57\n",
       "26    9898    961     2861    3151               242         833\n",
       "27   14276    803     3045     485               100         518\n",
       "28    4113  20484    25957    1158              8604        5206\n",
       "29   43088   2100     2609    1200              1107         823\n",
       "..     ...    ...      ...     ...               ...         ...\n",
       "410   6633   2096     4563    1389              1860        1892\n",
       "411   2126   3289     3281    1535               235        4365\n",
       "412     97   3605    12400      98              2970          62\n",
       "413   4983   4859     6633   17866               912        2435\n",
       "414   5969   1990     3417    5679              1135         290\n",
       "415   7842   6046     8552    1691              3540        1874\n",
       "416   4389  10940    10908     848              6728         993\n",
       "417   5065   5499    11055     364              3485        1063\n",
       "418    660   8494    18622     133              6740         776\n",
       "419   8861   3783     2223     633              1580        1521\n",
       "420   4456   5266    13227      25              6818        1393\n",
       "421  17063   4847     9053    1031              3415        1784\n",
       "422  26400   1377     4172     830               948        1218\n",
       "423  17565   3686     4657    1059              1803         668\n",
       "424  16980   2884    12232     874              3213         249\n",
       "425  11243   2408     2593   15348               108        1886\n",
       "426  13134   9347    14316    3141              5079        1894\n",
       "427  31012  16687     5429   15082               439        1163\n",
       "428   3047   5970     4910    2198               850         317\n",
       "429   8607   1750     3580      47                84        2501\n",
       "430   3097   4230    16483     575               241        2080\n",
       "431   8533   5506     5160   13486              1377        1498\n",
       "432  21117   1162     4754     269              1328         395\n",
       "433   1982   3218     1493    1541               356        1449\n",
       "434  16731   3922     7994     688              2371         838\n",
       "435  29703  12051    16027   13135               182        2204\n",
       "436  39228   1431      764    4510                93        2346\n",
       "437  14531  15488    30243     437             14841        1867\n",
       "438  10290   1981     2232    1038               168        2125\n",
       "439   2787   1698     2510      65               477          52\n",
       "\n",
       "[440 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels = raw_df['Channel']\n",
    "df = raw_df.drop(['Channel','Region'], axis = 1)\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get right down to it and begin our clustering analysis. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Import `KMeans` from `sklearn.cluster`, and then create an instance of it. Set the number of clusters to `2`\n",
    "* Fit the cluster object.\n",
    "* Get the predictions from the clustering algorithm and store them in `cluster_preds`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=2, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k_means = KMeans(n_clusters = 2)\n",
    "k_means.fit(df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_preds = k_means.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use some of the metrics we've learned about to check the performance of our segmentation. We'll use `calinski_harabaz_score` and `adjusted_rand_score`, which can both be found inside `sklearn.metrics.cluster`. \n",
    "\n",
    "In the cell below, import these scoring functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start with CH Score, to get the variance ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171.68461633384186"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.calinski_harabaz_score(df, cluster_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we don't have any other numbers to compare this to, this is a pretty low score, suggesting that our clusters aren't great. \n",
    "\n",
    "Since we actually have ground-truth labels in this case, we can actually use the `adjusted_rand_score` to tell us how well the clustering performed. Adjust Rand Score is meant to compare two clusterings, which the score can interpret our labels as. This will tell us how similar our predicted clusters are to the actual channels. \n",
    "\n",
    "Adjusted Rand Score is bounded between -1 and 1. A score close to 1 shows that the clusters are almost identical. A score close to 0 means that predictions are essentially random, while a score close to -1 means that the predictions are pathologically bad, since they are worse than random chance. \n",
    "\n",
    "In the cell below, call `adjusted_rand_score` and pass in our `channels` and `cluster_preds` to see how well our first iteration of clustering did. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.03060891241109425"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.adjusted_rand_score(cluster_preds, channels)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to these results, our clusterings were essentially no better than random chance. Let's see if we can improve this. \n",
    "\n",
    "### Scaling Our Dataset\n",
    "\n",
    "Recall that the results of K-Means Clustering is heavily affected by scaling. Since the clustering algorithm is distance-based, this makes sense. Let's use a `StandardScaler` object to scale our dataset and then try our clustering again and see if the results are different. \n",
    "\n",
    "In the cells below:\n",
    "\n",
    "* Import a [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) object and use it to transform our dataset. \n",
    "* Create another K-Means object, fit it to our scaled data, and then use it to predict clusters.\n",
    "* Calculate the Adjusted Rand Score of our new predictions and our labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/learn-env/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/conda/envs/learn-env/lib/python3.6/site-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.052933</td>\n",
       "      <td>0.523568</td>\n",
       "      <td>-0.041115</td>\n",
       "      <td>-0.589367</td>\n",
       "      <td>-0.043569</td>\n",
       "      <td>-0.066339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.391302</td>\n",
       "      <td>0.544458</td>\n",
       "      <td>0.170318</td>\n",
       "      <td>-0.270136</td>\n",
       "      <td>0.086407</td>\n",
       "      <td>0.089151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.447029</td>\n",
       "      <td>0.408538</td>\n",
       "      <td>-0.028157</td>\n",
       "      <td>-0.137536</td>\n",
       "      <td>0.133232</td>\n",
       "      <td>2.243293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.100111</td>\n",
       "      <td>-0.624020</td>\n",
       "      <td>-0.392977</td>\n",
       "      <td>0.687144</td>\n",
       "      <td>-0.498588</td>\n",
       "      <td>0.093411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.840239</td>\n",
       "      <td>-0.052396</td>\n",
       "      <td>-0.079356</td>\n",
       "      <td>0.173859</td>\n",
       "      <td>-0.231918</td>\n",
       "      <td>1.299347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.204806</td>\n",
       "      <td>0.334067</td>\n",
       "      <td>-0.297637</td>\n",
       "      <td>-0.496155</td>\n",
       "      <td>-0.228138</td>\n",
       "      <td>-0.026224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.009950</td>\n",
       "      <td>-0.352316</td>\n",
       "      <td>-0.102849</td>\n",
       "      <td>-0.534512</td>\n",
       "      <td>0.054280</td>\n",
       "      <td>-0.347854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.349981</td>\n",
       "      <td>-0.113981</td>\n",
       "      <td>0.155359</td>\n",
       "      <td>-0.289315</td>\n",
       "      <td>0.092286</td>\n",
       "      <td>0.369601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.477901</td>\n",
       "      <td>-0.291409</td>\n",
       "      <td>-0.185336</td>\n",
       "      <td>-0.545854</td>\n",
       "      <td>-0.244726</td>\n",
       "      <td>-0.275079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.474497</td>\n",
       "      <td>0.718495</td>\n",
       "      <td>1.151423</td>\n",
       "      <td>-0.394488</td>\n",
       "      <td>0.954031</td>\n",
       "      <td>0.203461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.683474</td>\n",
       "      <td>-0.053346</td>\n",
       "      <td>0.529133</td>\n",
       "      <td>0.273876</td>\n",
       "      <td>0.649984</td>\n",
       "      <td>0.077791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.090692</td>\n",
       "      <td>-0.633787</td>\n",
       "      <td>-0.361162</td>\n",
       "      <td>-0.340664</td>\n",
       "      <td>-0.489769</td>\n",
       "      <td>-0.364894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.560499</td>\n",
       "      <td>0.884800</td>\n",
       "      <td>0.400925</td>\n",
       "      <td>-0.574313</td>\n",
       "      <td>0.209873</td>\n",
       "      <td>0.499176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.729576</td>\n",
       "      <td>0.055851</td>\n",
       "      <td>0.740672</td>\n",
       "      <td>0.004757</td>\n",
       "      <td>0.803267</td>\n",
       "      <td>-0.327619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.001564</td>\n",
       "      <td>0.497659</td>\n",
       "      <td>0.436111</td>\n",
       "      <td>-0.572869</td>\n",
       "      <td>0.457016</td>\n",
       "      <td>0.228311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.138313</td>\n",
       "      <td>-0.635143</td>\n",
       "      <td>-0.435116</td>\n",
       "      <td>-0.551629</td>\n",
       "      <td>-0.402629</td>\n",
       "      <td>-0.395069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.869179</td>\n",
       "      <td>0.409623</td>\n",
       "      <td>0.439272</td>\n",
       "      <td>-0.605865</td>\n",
       "      <td>0.341529</td>\n",
       "      <td>-0.157929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.484788</td>\n",
       "      <td>0.048933</td>\n",
       "      <td>-0.528665</td>\n",
       "      <td>-0.460479</td>\n",
       "      <td>-0.527355</td>\n",
       "      <td>1.048362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.522499</td>\n",
       "      <td>0.071993</td>\n",
       "      <td>0.226258</td>\n",
       "      <td>-0.178780</td>\n",
       "      <td>-0.024041</td>\n",
       "      <td>0.587926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.334071</td>\n",
       "      <td>-0.447812</td>\n",
       "      <td>0.159362</td>\n",
       "      <td>-0.495536</td>\n",
       "      <td>-0.076325</td>\n",
       "      <td>-0.363474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.438987</td>\n",
       "      <td>-0.173259</td>\n",
       "      <td>-0.352839</td>\n",
       "      <td>-0.413666</td>\n",
       "      <td>-0.130709</td>\n",
       "      <td>0.212691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.509248</td>\n",
       "      <td>-0.668106</td>\n",
       "      <td>-0.625901</td>\n",
       "      <td>0.064149</td>\n",
       "      <td>-0.526305</td>\n",
       "      <td>-0.339334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.525828</td>\n",
       "      <td>-0.526217</td>\n",
       "      <td>-0.366851</td>\n",
       "      <td>1.306634</td>\n",
       "      <td>-0.105092</td>\n",
       "      <td>0.997242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.137716</td>\n",
       "      <td>4.154476</td>\n",
       "      <td>1.482005</td>\n",
       "      <td>0.429367</td>\n",
       "      <td>0.305623</td>\n",
       "      <td>5.324340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.842773</td>\n",
       "      <td>0.539846</td>\n",
       "      <td>0.615308</td>\n",
       "      <td>-0.032363</td>\n",
       "      <td>0.336069</td>\n",
       "      <td>1.509862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.329670</td>\n",
       "      <td>-0.212462</td>\n",
       "      <td>-0.037533</td>\n",
       "      <td>-0.592048</td>\n",
       "      <td>0.235490</td>\n",
       "      <td>-0.521094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.166414</td>\n",
       "      <td>-0.655897</td>\n",
       "      <td>-0.536250</td>\n",
       "      <td>0.016306</td>\n",
       "      <td>-0.554232</td>\n",
       "      <td>-0.245614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.180140</td>\n",
       "      <td>-0.677330</td>\n",
       "      <td>-0.516866</td>\n",
       "      <td>-0.533481</td>\n",
       "      <td>-0.584049</td>\n",
       "      <td>-0.357439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.624343</td>\n",
       "      <td>1.992372</td>\n",
       "      <td>1.896865</td>\n",
       "      <td>-0.394694</td>\n",
       "      <td>1.201593</td>\n",
       "      <td>1.306802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.460843</td>\n",
       "      <td>-0.501394</td>\n",
       "      <td>-0.562798</td>\n",
       "      <td>-0.386033</td>\n",
       "      <td>-0.372602</td>\n",
       "      <td>-0.249164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>-0.424865</td>\n",
       "      <td>-0.501936</td>\n",
       "      <td>-0.356948</td>\n",
       "      <td>-0.347057</td>\n",
       "      <td>-0.214490</td>\n",
       "      <td>0.130331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>-0.781630</td>\n",
       "      <td>-0.340107</td>\n",
       "      <td>-0.492004</td>\n",
       "      <td>-0.316948</td>\n",
       "      <td>-0.555702</td>\n",
       "      <td>1.008247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>-0.942242</td>\n",
       "      <td>-0.297242</td>\n",
       "      <td>0.468664</td>\n",
       "      <td>-0.613289</td>\n",
       "      <td>0.018584</td>\n",
       "      <td>-0.519319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>-0.555476</td>\n",
       "      <td>-0.127139</td>\n",
       "      <td>-0.138878</td>\n",
       "      <td>3.050856</td>\n",
       "      <td>-0.413548</td>\n",
       "      <td>0.323096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>-0.477426</td>\n",
       "      <td>-0.516315</td>\n",
       "      <td>-0.477677</td>\n",
       "      <td>0.537634</td>\n",
       "      <td>-0.366723</td>\n",
       "      <td>-0.438379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>-0.329163</td>\n",
       "      <td>0.033876</td>\n",
       "      <td>0.063285</td>\n",
       "      <td>-0.284778</td>\n",
       "      <td>0.138271</td>\n",
       "      <td>0.123941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>-0.602496</td>\n",
       "      <td>0.697741</td>\n",
       "      <td>0.311485</td>\n",
       "      <td>-0.458623</td>\n",
       "      <td>0.807677</td>\n",
       "      <td>-0.188814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>-0.548985</td>\n",
       "      <td>-0.040324</td>\n",
       "      <td>0.326971</td>\n",
       "      <td>-0.558434</td>\n",
       "      <td>0.126722</td>\n",
       "      <td>-0.163964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>-0.897676</td>\n",
       "      <td>0.365944</td>\n",
       "      <td>1.124138</td>\n",
       "      <td>-0.606071</td>\n",
       "      <td>0.810196</td>\n",
       "      <td>-0.265849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>-0.248501</td>\n",
       "      <td>-0.273097</td>\n",
       "      <td>-0.603462</td>\n",
       "      <td>-0.502960</td>\n",
       "      <td>-0.273283</td>\n",
       "      <td>-0.001374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>-0.597192</td>\n",
       "      <td>-0.071930</td>\n",
       "      <td>0.555786</td>\n",
       "      <td>-0.628343</td>\n",
       "      <td>0.826575</td>\n",
       "      <td>-0.046814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>0.400754</td>\n",
       "      <td>-0.128767</td>\n",
       "      <td>0.116064</td>\n",
       "      <td>-0.420884</td>\n",
       "      <td>0.112024</td>\n",
       "      <td>0.091991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>1.139853</td>\n",
       "      <td>-0.599468</td>\n",
       "      <td>-0.398139</td>\n",
       "      <td>-0.462335</td>\n",
       "      <td>-0.405989</td>\n",
       "      <td>-0.108939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>0.440491</td>\n",
       "      <td>-0.286255</td>\n",
       "      <td>-0.347045</td>\n",
       "      <td>-0.415110</td>\n",
       "      <td>-0.226458</td>\n",
       "      <td>-0.304189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>0.394184</td>\n",
       "      <td>-0.395045</td>\n",
       "      <td>0.450965</td>\n",
       "      <td>-0.453261</td>\n",
       "      <td>0.069609</td>\n",
       "      <td>-0.452934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>-0.059946</td>\n",
       "      <td>-0.459614</td>\n",
       "      <td>-0.564483</td>\n",
       "      <td>2.531590</td>\n",
       "      <td>-0.582369</td>\n",
       "      <td>0.128201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>0.089742</td>\n",
       "      <td>0.481652</td>\n",
       "      <td>0.670510</td>\n",
       "      <td>0.014243</td>\n",
       "      <td>0.461425</td>\n",
       "      <td>0.131041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>1.504930</td>\n",
       "      <td>1.477314</td>\n",
       "      <td>-0.265717</td>\n",
       "      <td>2.476735</td>\n",
       "      <td>-0.512867</td>\n",
       "      <td>-0.128464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>-0.708726</td>\n",
       "      <td>0.023567</td>\n",
       "      <td>-0.320392</td>\n",
       "      <td>-0.180224</td>\n",
       "      <td>-0.426566</td>\n",
       "      <td>-0.428794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>-0.268607</td>\n",
       "      <td>-0.548871</td>\n",
       "      <td>-0.460505</td>\n",
       "      <td>-0.623806</td>\n",
       "      <td>-0.587408</td>\n",
       "      <td>0.346526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>-0.704768</td>\n",
       "      <td>-0.212462</td>\n",
       "      <td>0.898799</td>\n",
       "      <td>-0.514921</td>\n",
       "      <td>-0.554442</td>\n",
       "      <td>0.197071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>-0.274465</td>\n",
       "      <td>-0.039374</td>\n",
       "      <td>-0.294055</td>\n",
       "      <td>2.147605</td>\n",
       "      <td>-0.315909</td>\n",
       "      <td>-0.009539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>0.721661</td>\n",
       "      <td>-0.628632</td>\n",
       "      <td>-0.336826</td>\n",
       "      <td>-0.578025</td>\n",
       "      <td>-0.326197</td>\n",
       "      <td>-0.401104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>-0.793029</td>\n",
       "      <td>-0.349738</td>\n",
       "      <td>-0.680366</td>\n",
       "      <td>-0.315711</td>\n",
       "      <td>-0.530295</td>\n",
       "      <td>-0.026934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>0.374473</td>\n",
       "      <td>-0.254242</td>\n",
       "      <td>0.004501</td>\n",
       "      <td>-0.491618</td>\n",
       "      <td>-0.107192</td>\n",
       "      <td>-0.243839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>1.401312</td>\n",
       "      <td>0.848446</td>\n",
       "      <td>0.850760</td>\n",
       "      <td>2.075222</td>\n",
       "      <td>-0.566831</td>\n",
       "      <td>0.241091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>2.155293</td>\n",
       "      <td>-0.592142</td>\n",
       "      <td>-0.757165</td>\n",
       "      <td>0.296561</td>\n",
       "      <td>-0.585519</td>\n",
       "      <td>0.291501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.200326</td>\n",
       "      <td>1.314671</td>\n",
       "      <td>2.348386</td>\n",
       "      <td>-0.543380</td>\n",
       "      <td>2.511218</td>\n",
       "      <td>0.121456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>-0.135384</td>\n",
       "      <td>-0.517536</td>\n",
       "      <td>-0.602514</td>\n",
       "      <td>-0.419441</td>\n",
       "      <td>-0.569770</td>\n",
       "      <td>0.213046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>-0.729307</td>\n",
       "      <td>-0.555924</td>\n",
       "      <td>-0.573227</td>\n",
       "      <td>-0.620094</td>\n",
       "      <td>-0.504888</td>\n",
       "      <td>-0.522869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>440 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5\n",
       "0    0.052933  0.523568 -0.041115 -0.589367 -0.043569 -0.066339\n",
       "1   -0.391302  0.544458  0.170318 -0.270136  0.086407  0.089151\n",
       "2   -0.447029  0.408538 -0.028157 -0.137536  0.133232  2.243293\n",
       "3    0.100111 -0.624020 -0.392977  0.687144 -0.498588  0.093411\n",
       "4    0.840239 -0.052396 -0.079356  0.173859 -0.231918  1.299347\n",
       "5   -0.204806  0.334067 -0.297637 -0.496155 -0.228138 -0.026224\n",
       "6    0.009950 -0.352316 -0.102849 -0.534512  0.054280 -0.347854\n",
       "7   -0.349981 -0.113981  0.155359 -0.289315  0.092286  0.369601\n",
       "8   -0.477901 -0.291409 -0.185336 -0.545854 -0.244726 -0.275079\n",
       "9   -0.474497  0.718495  1.151423 -0.394488  0.954031  0.203461\n",
       "10  -0.683474 -0.053346  0.529133  0.273876  0.649984  0.077791\n",
       "11   0.090692 -0.633787 -0.361162 -0.340664 -0.489769 -0.364894\n",
       "12   1.560499  0.884800  0.400925 -0.574313  0.209873  0.499176\n",
       "13   0.729576  0.055851  0.740672  0.004757  0.803267 -0.327619\n",
       "14   1.001564  0.497659  0.436111 -0.572869  0.457016  0.228311\n",
       "15  -0.138313 -0.635143 -0.435116 -0.551629 -0.402629 -0.395069\n",
       "16  -0.869179  0.409623  0.439272 -0.605865  0.341529 -0.157929\n",
       "17  -0.484788  0.048933 -0.528665 -0.460479 -0.527355  1.048362\n",
       "18   0.522499  0.071993  0.226258 -0.178780 -0.024041  0.587926\n",
       "19  -0.334071 -0.447812  0.159362 -0.495536 -0.076325 -0.363474\n",
       "20   0.438987 -0.173259 -0.352839 -0.413666 -0.130709  0.212691\n",
       "21  -0.509248 -0.668106 -0.625901  0.064149 -0.526305 -0.339334\n",
       "22   1.525828 -0.526217 -0.366851  1.306634 -0.105092  0.997242\n",
       "23   1.137716  4.154476  1.482005  0.429367  0.305623  5.324340\n",
       "24   0.842773  0.539846  0.615308 -0.032363  0.336069  1.509862\n",
       "25   0.329670 -0.212462 -0.037533 -0.592048  0.235490 -0.521094\n",
       "26  -0.166414 -0.655897 -0.536250  0.016306 -0.554232 -0.245614\n",
       "27   0.180140 -0.677330 -0.516866 -0.533481 -0.584049 -0.357439\n",
       "28  -0.624343  1.992372  1.896865 -0.394694  1.201593  1.306802\n",
       "29   2.460843 -0.501394 -0.562798 -0.386033 -0.372602 -0.249164\n",
       "..        ...       ...       ...       ...       ...       ...\n",
       "410 -0.424865 -0.501936 -0.356948 -0.347057 -0.214490  0.130331\n",
       "411 -0.781630 -0.340107 -0.492004 -0.316948 -0.555702  1.008247\n",
       "412 -0.942242 -0.297242  0.468664 -0.613289  0.018584 -0.519319\n",
       "413 -0.555476 -0.127139 -0.138878  3.050856 -0.413548  0.323096\n",
       "414 -0.477426 -0.516315 -0.477677  0.537634 -0.366723 -0.438379\n",
       "415 -0.329163  0.033876  0.063285 -0.284778  0.138271  0.123941\n",
       "416 -0.602496  0.697741  0.311485 -0.458623  0.807677 -0.188814\n",
       "417 -0.548985 -0.040324  0.326971 -0.558434  0.126722 -0.163964\n",
       "418 -0.897676  0.365944  1.124138 -0.606071  0.810196 -0.265849\n",
       "419 -0.248501 -0.273097 -0.603462 -0.502960 -0.273283 -0.001374\n",
       "420 -0.597192 -0.071930  0.555786 -0.628343  0.826575 -0.046814\n",
       "421  0.400754 -0.128767  0.116064 -0.420884  0.112024  0.091991\n",
       "422  1.139853 -0.599468 -0.398139 -0.462335 -0.405989 -0.108939\n",
       "423  0.440491 -0.286255 -0.347045 -0.415110 -0.226458 -0.304189\n",
       "424  0.394184 -0.395045  0.450965 -0.453261  0.069609 -0.452934\n",
       "425 -0.059946 -0.459614 -0.564483  2.531590 -0.582369  0.128201\n",
       "426  0.089742  0.481652  0.670510  0.014243  0.461425  0.131041\n",
       "427  1.504930  1.477314 -0.265717  2.476735 -0.512867 -0.128464\n",
       "428 -0.708726  0.023567 -0.320392 -0.180224 -0.426566 -0.428794\n",
       "429 -0.268607 -0.548871 -0.460505 -0.623806 -0.587408  0.346526\n",
       "430 -0.704768 -0.212462  0.898799 -0.514921 -0.554442  0.197071\n",
       "431 -0.274465 -0.039374 -0.294055  2.147605 -0.315909 -0.009539\n",
       "432  0.721661 -0.628632 -0.336826 -0.578025 -0.326197 -0.401104\n",
       "433 -0.793029 -0.349738 -0.680366 -0.315711 -0.530295 -0.026934\n",
       "434  0.374473 -0.254242  0.004501 -0.491618 -0.107192 -0.243839\n",
       "435  1.401312  0.848446  0.850760  2.075222 -0.566831  0.241091\n",
       "436  2.155293 -0.592142 -0.757165  0.296561 -0.585519  0.291501\n",
       "437  0.200326  1.314671  2.348386 -0.543380  2.511218  0.121456\n",
       "438 -0.135384 -0.517536 -0.602514 -0.419441 -0.569770  0.213046\n",
       "439 -0.729307 -0.555924 -0.573227 -0.620094 -0.504888 -0.522869\n",
       "\n",
       "[440 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaled = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaled.fit_transform(df,channels))\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means = KMeans(n_clusters = 2)\n",
    "k_means.fit(df_scaled)\n",
    "cluster_preds = k_means.predict(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20072918876202783"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.adjusted_rand_score(cluster_preds, channels)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a big improvement! Although it's not perfect, we can see that scaling our data had a significant effect on the quality of our clusters. \n",
    "\n",
    "## Incorporating PCA\n",
    "\n",
    "Since clustering algorithms are distance-based, this means that dimensionality has a definite effect on their performance. The greater the dimensionality of the dataset, the the greater the total area that we have to worry about our clusters existing in. Let's try using some Principal Component Analysis to transform our data and see if this affects the performance of our clustering algorithm. \n",
    "\n",
    "Since you've already seen PCA in a previous section, we won't hold your hand through section too much. \n",
    "\n",
    "In the cells below:\n",
    "\n",
    "* Import [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) from the appropriate module in sklearn\n",
    "* Create a `PCA` instance and use it to tranform our scaled data. \n",
    "* Investigate the explained variance ratio for each Principal Component. Consider dropping certain components to reduce dimensionality if you feel it is worth the loss of information.\n",
    "* Create a new `KMeans` object, fit it to our pca-transformed data, and check the Adjusted Rand Score of the predictions it makes. \n",
    "\n",
    "**_NOTE:_** Your overall goal here is to get the highest possible Adjusted Rand Score. Don't be afraid to change parameters and rerun things to see how it changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4) #choosing number of features wanted\n",
    "transformed = pca.fit_transform(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44082893, 0.283764  , 0.12334413, 0.09395504])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_ #Overall variance in the dataset accounted for in each principle component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44082893, 0.72459292, 0.84793705, 0.94189209])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cumsum(pca.explained_variance_ratio_) #Accounting for the variance not in our primary componenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means = KMeans(n_clusters=3)\n",
    "k_means.fit(transformed)\n",
    "cluster_preds_transformed = k_means.predict(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2344470496484786"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.adjusted_rand_score(cluster_preds_transformed, channels)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Question_**:  What was the Highest Adjusted Rand Score you achieved? Interpret this score, and determine the overall quality of the clustering. Did PCA affect the performance overall?  How many Principal Components resulted in the best overall clustering performance? Why do you think this is?\n",
    "\n",
    "Write your answer below this line:\n",
    "_______________________________________________________________________________________________________________________________\n",
    "\n",
    "The highest ARS should be ~0.23, which suggests that the clusters are better than random chance, but far from perfect.  Overall, the quality of the clustering algorithm did a lot better than the first algorithm we ran on unscaled data. The best performance was achieved when reducing the number of Principal Components down to 4. The increase in model performance is likely due to the reduction in dimensionality. Although dropping the last 2 PCs means that we lose about 6% of our explained variance, this proved to be a net-positive trade-off for the reduction in dimensionality it provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Step: Hierarchical Agglomerative Clustering\n",
    "\n",
    "Now that we've tried doing market segmentation with K-Means Clustering, let's end this lab by trying with HAC!\n",
    "\n",
    "In the cells below, use [Agglomerative Clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) to make cluster predictions on the datasets we've created, and see how HAC's performance compares to K-Mean's performance. \n",
    "\n",
    "**_NOTE_**: Don't just try HAC on the PCA-transformed dataset--also compare algorithm performance on the scaled and unscaled datasets, as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-4be241dfc1d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mAC_NotScaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Not scaled HAC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAC_NotScaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rainbow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/learn-env/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/learn-env/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/learn-env/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2485\u001b[0m         \u001b[0;34m\"\"\"Return the cached item, item represents a label indexer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2486\u001b[0m         \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_item_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2487\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2489\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x504 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "AC_NotScaled = AgglomerativeClustering(n_clusters=3, affinity = 'euclidean', linkage = 'ward')\n",
    "AC_NotScaled.fit_predict(df) #Not scaled HAC\n",
    "plt.figure(figsize=(10, 7))  \n",
    "plt.scatter(df[:,0], df[:,1], c=AC_NotScaled.labels_, cmap='rainbow')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1,\n",
       "       1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 0, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 0, 2, 2, 1, 2,\n",
       "       2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 0, 0, 1,\n",
       "       1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2,\n",
       "       1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2,\n",
       "       1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 2,\n",
       "       1, 1, 2, 1, 1, 1, 2, 0, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2,\n",
       "       1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1,\n",
       "       1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "       2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1,\n",
       "       1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
       "       1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2,\n",
       "       1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1,\n",
       "       1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2,\n",
       "       2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AC_Scaled = AgglomerativeClustering(n_clusters=3, affinity = 'euclidean', linkage = 'ward')\n",
    "AC_Scaled.fit_predict(df_scaled) #Scaled HAC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 2, 0, 0, 0, 2, 2, 2, 0, 2, 0, 2,\n",
       "       0, 2, 0, 0, 2, 0, 1, 0, 0, 2, 0, 0, 2, 2, 0, 2, 1, 0, 0, 0, 2, 1,\n",
       "       2, 1, 1, 1, 2, 1, 2, 2, 0, 2, 0, 2, 1, 2, 0, 2, 2, 1, 2, 2, 2, 1,\n",
       "       2, 0, 2, 2, 0, 0, 2, 0, 2, 0, 2, 1, 2, 2, 2, 2, 2, 0, 2, 1, 1, 0,\n",
       "       2, 0, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 1,\n",
       "       2, 2, 0, 0, 0, 2, 2, 2, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 2, 0, 2, 2,\n",
       "       0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 0, 1, 2, 2, 2, 0, 0, 2, 0, 2,\n",
       "       2, 1, 2, 0, 2, 2, 2, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2,\n",
       "       0, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 2, 2, 2, 0, 0, 2, 1, 2, 0, 0, 2,\n",
       "       2, 2, 1, 1, 0, 2, 2, 1, 2, 2, 2, 1, 0, 1, 2, 2, 2, 2, 1, 0, 2, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 0, 2, 0, 0, 0,\n",
       "       0, 2, 2, 2, 2, 0, 0, 2, 2, 1, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 0, 2,\n",
       "       2, 2, 1, 0, 2, 0, 2, 2, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2, 0, 0, 0, 0,\n",
       "       2, 0, 0, 0, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 0, 1, 2, 2, 1, 2, 1, 0,\n",
       "       2, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 0, 0, 0, 0, 2, 2, 0, 2,\n",
       "       2, 1, 0, 1, 0, 0, 0, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 0, 2, 1, 2, 1,\n",
       "       2, 1, 2, 2, 0, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0,\n",
       "       2, 2, 2, 0, 2, 2, 0, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2,\n",
       "       2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       1, 2, 2, 0, 0, 0, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2, 0, 0, 0, 1, 2, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AC_NotScaled.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, we used our knowledge of clustering to perform a market segmentation on a real-world dataset. We started with a cluster analysis with poor performance, and then implemented some changes to iteratively improve the performance of the clustering analysis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
